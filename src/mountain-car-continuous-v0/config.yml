run:
  file_dir: "temp/trash"

env:
  name: MountainCarContinuous-v0
  max_step: 200

value:
  n_sim: 1000
  max_pos_episodes: null # Optional[int]
  at_most_n_neg_episodes_per_pos: null # Optional[float]
  reward:
    max_vel_weight: 1.
    min_vel_weight: 0.25
    max_dist_weight: 0.5
    min_dist_weight: 0.125
    discount: 0.5
    end_reward_scale: 0. # 100.
    step_reward_scale: 1.
  load_dataset: True
  val_split: 0.1
  extreme_action_chance: 1.0
  epochs: 10000
  model:
    load: true
    layers:
      - type: dense
        units: 8
        activation: relu
      - type: dense
        units: 8
        activation: relu
      - type: dense
        units: 4
        activation: relu
      - type: dense
        units: 1
        activation: linear
  lr: 1.e-5
  min_delta: 1.e-7
  monitor: val_loss
  label_scale: 1.e-2
  batch_size: 50
  initial_score: true
  score_period: 10

policy:
  skip: true
  n_sim: 10
  n_policy_samples: 2000
  n_action_samples: 20
  load_dataset: true
  epochs: 100
  lr: 1.e-5
  min_delta: 1.e-5
  monitor: loss
  batch_size: 50
  score_period: 10

eval:
  n_sim: 5
  render_every_n_step: null # Optional[int]

